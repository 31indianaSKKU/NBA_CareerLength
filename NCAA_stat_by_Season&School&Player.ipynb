{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm94zRLXe1UX1ESqsy/0ib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/31indianaSKKU/NBA_CareerLength/blob/main/NCAA_stat_by_Season%26School%26Player.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Jupyter"
      ],
      "metadata": {
        "id": "FLkG3zyovt-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import os\n",
        "\n",
        "def get_season_from_page(soup):\n",
        "    season_element = soup.select_one(\"#meta div h1 span\")\n",
        "    if season_element:\n",
        "        return season_element.text.strip().split()[0]\n",
        "    return \"\"\n",
        "\n",
        "def get_school_name(url):\n",
        "    url_parts = url.split(\"/\")\n",
        "    return url_parts[-3]\n",
        "\n",
        "\n",
        "def get_table_data(table):\n",
        "    header_row = [cell.text.strip() for cell in table.select(\"thead tr th\")]\n",
        "    data_rows = table.select(\"tbody tr\")\n",
        "    data = [[cell.text.strip() for cell in row.select(\"th, td\")] for row in data_rows]\n",
        "    return pd.DataFrame(data, columns=header_row)\n",
        "\n",
        "\n",
        "for yrs in range(2000,2001):  #season지정\n",
        "    ratings_url = f\"https://www.sports-reference.com/cbb/seasons/men/{yrs}-ratings.html\" \n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    base_url = \"https://www.sports-reference.com/\"\n",
        "\n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    table = soup.find(\"table\", id=\"ratings\")\n",
        "#     rows = table.select(\"tbody tr\")\n",
        "    rows = table.select(\"tbody tr\")[:2]  # 첫 5개 행만 선택_test용\n",
        "\n",
        "    # 빈 데이터프레임 생성\n",
        "    combined_df = pd.DataFrame()\n",
        "   # 필요한 컬럼들을 리스트로 저장\n",
        "    columns_to_drop = [\"#\", \"Class\", \"Pos\", \"Height\", \"Weight\", \"Hometown\", \"High School\", \"RSCI Top 100\", \"Summary\"]\n",
        "\n",
        "    for row in rows:\n",
        "        link_element = row.select_one(\"td:nth-of-type(1) a\")\n",
        "        if link_element:\n",
        "            link = link_element[\"href\"]\n",
        "            full_url = base_url + link\n",
        "\n",
        "            response = requests.get(full_url)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            roster_table = soup.find(\"table\", id=\"roster\")\n",
        "            per_game_table = soup.find(\"table\", id=\"per_game\")\n",
        "            totals_table = soup.find(\"table\", id=\"totals\")\n",
        "            per_min_table = soup.find(\"table\", id=\"per_min\")\n",
        "            per_poss_table = soup.find(\"table\", id=\"per_poss\")\n",
        "            advanced_table = soup.find(\"table\", id=\"advanced\")\n",
        "\n",
        "            data_frames = list()\n",
        "\n",
        "            if roster_table is not None:\n",
        "                roster_data = get_table_data(roster_table)\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                roster_data[\"Season\"] = season\n",
        "                roster_data[\"School\"] = school\n",
        "                data_frames.append(roster_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Roster table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_game_table is not None:\n",
        "                per_game_data = get_table_data(per_game_table)\n",
        "                per_game_data = per_game_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_game\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_game_data[\"Season\"] = season\n",
        "                per_game_data[\"School\"] = school\n",
        "                data_frames.append(per_game_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} PerGame table not found.\")\n",
        "                pass\n",
        "\n",
        "            if totals_table is not None:\n",
        "                totals_data = get_table_data(totals_table)\n",
        "                totals_data = totals_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_totals\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                totals_data[\"Season\"] = season\n",
        "                totals_data[\"School\"] = school\n",
        "                # print(\"Totals data frame:\")\n",
        "                # print(totals_data.head())\n",
        "                data_frames.append(totals_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Totals table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_min_table is not None:\n",
        "                per_min_data = get_table_data(per_min_table)\n",
        "                per_min_data = per_min_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_min\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_min_data[\"Season\"] = season\n",
        "                per_min_data[\"School\"] = school\n",
        "                # print(\"Per 40 Min data frame:\")\n",
        "                # print(per_min_data.head())\n",
        "                data_frames.append(per_min_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Per40Min table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_poss_table is not None:\n",
        "                per_poss_data = get_table_data(per_poss_table)\n",
        "                per_poss_data = per_poss_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_poss\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_poss_data[\"Season\"] = season\n",
        "                per_poss_data[\"School\"] = school\n",
        "                # print(\"Per 100 Poss data frame:\")\n",
        "                # print(per_poss_data.head())\n",
        "                data_frames.append(per_poss_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Per100Poss table not found.\")\n",
        "                pass\n",
        "\n",
        "            if advanced_table is not None:\n",
        "                advanced_data = get_table_data(advanced_table)\n",
        "                advanced_data = advanced_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_advanced\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                advanced_data[\"Season\"] = season \n",
        "                advanced_data[\"School\"] = school\n",
        "                # print(\"Advanced data frame:\")\n",
        "                # print(advanced_data.head())\n",
        "                data_frames.append(advanced_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Advanced table not found.\")\n",
        "                pass\n",
        "\n",
        "            combined_df = roster_data.copy()\n",
        "            for col in columns_to_drop:\n",
        "                try:\n",
        "                    combined_df = combined_df.drop(col, axis=1)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for df in data_frames:\n",
        "                if df is not None:\n",
        "                    df = df.loc[:, ~df.columns.duplicated()]\n",
        "                    combined_df = pd.merge(combined_df, df, on=[\"Player\", \"Season\", \"School\"], how=\"outer\")\n",
        "\n",
        "            combined_df = combined_df[[\"Season\", \"School\"] + [col for col in combined_df.columns if col not in [\"Season\", \"School\"]]]\n",
        "            # Season 값 형식 변경\n",
        "            # combined_df[\"Season\"] = combined_df[\"Season\"].apply(lambda x: f\"{x[:4]}-{x[-2:]}\")\n",
        "            combined_df[\"Season\"] = combined_df[\"Season\"].astype(str)\n",
        "\n",
        "            # 파일명 생성\n",
        "            filename = \"NCAA_\" + season + \".csv\"\n",
        "            #  # 빈 데이터프레임을 CSV 파일로 저장\n",
        "            # df.to_csv(filename, index=False)\n",
        "\n",
        "            # 파일이 이미 존재하는 경우\n",
        "            if filename in os.listdir():\n",
        "                # 기존 파일 읽어오기\n",
        "                existing_df = pd.read_csv(filename)\n",
        "                # 기존 데이터와 현재 데이터 병합\n",
        "                combined_df = pd.concat([existing_df, combined_df], ignore_index=False)\n",
        "            # else:\n",
        "                # combined_df = pd.concat([combined_df] + data_frames, ignore_index=False)\n",
        "\n",
        "            # CSV 파일로 저장\n",
        "            combined_df.to_csv(filename, index=False)\n",
        "            print(\"Combined DataFrame has been saved as\", filename)\n",
        "\n",
        "            # 일정 시간 대기\n",
        "            time.sleep(5)  # 5초 대기\n",
        "\n",
        "print(\"Crawling complete!\")"
      ],
      "metadata": {
        "id": "AGELUutOvs4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vRdAHCzwvrdG"
      }
    }
  ]
}