{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO78G+tVrlP3GXr/RaN6jPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/31indianaSKKU/NBA_CareerLength/blob/main/NCAA_Stat_by_Season%26School%26Player_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lcZnSjw4qwQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 2000년대에서만 발생하는 문제\n",
        "-> 2000년대와 최근 테이블의 컬럼이 달라서 예외처리가 제대로 동작하지 않았습니다.\n",
        "-> -> merge하기 전에 컬럼을 드랍하여 번거로운 rename과정을 제거\n",
        "-> -> 한번에 drop하면 없는 컬럼에서 오류가 발생하기에, 하나씩 돌면서 에러가 발생하더라도 넘어가도록 처리\n",
        "\n",
        "* Season컴럼의 표시형식 문제는 재현되지 않아 확인 하지 못햇습니다.\n",
        "-> 혹시 엑셀로 csv를 보고 계시다면 엑셀 이슈일 수 있습니다.\n",
        "-> 데이터에는 이상 없어 보입니다.\n"
      ],
      "metadata": {
        "id": "XDdOC-Bq4tYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NCAA데이터 크롤링 완성\n",
        "#지정된 연도에 대한 모든 학교의 모든 선수의 모든 데이터 complete NCAA데이터 크롤링 완성\n",
        "\n",
        "#위의 코드 문제점\n",
        "#1. 2000년대 시즌 error: combined_df = combined_df.drop([\"#_y\", \"Class_y\", \"Pos_y\", \"Height_y\", \"Weight_y\", \"Hometown_y\", \"High School_y\", \"RSCI Top 100_y\", \"Summary_y\"], axis=1)\n",
        "#2. Season컬럼이 표시 형식이 2009-10시즌의 경우 Oct-09 식으로 표시됨.\n",
        "#3. 현재 1의 error는 drop할 컬럼명을 먼저 리스트로 만들어 놓고 컬럼명에 해당 컬럼이 있을 경우 삭제 처리하려고 시도 중이나 반영안되고 있음.\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import os\n",
        "\n",
        "def get_season_from_page(soup):\n",
        "    season_element = soup.select_one(\"#meta div h1 span\")\n",
        "    if season_element:\n",
        "        return season_element.text.strip().split()[0]\n",
        "    return \"\"\n",
        "\n",
        "def get_school_name(url):\n",
        "    url_parts = url.split(\"/\")\n",
        "    return url_parts[-3]\n",
        "\n",
        "\n",
        "def get_table_data(table):\n",
        "    header_row = [cell.text.strip() for cell in table.select(\"thead tr th\")]\n",
        "    data_rows = table.select(\"tbody tr\")\n",
        "    data = [[cell.text.strip() for cell in row.select(\"th, td\")] for row in data_rows]\n",
        "    return pd.DataFrame(data, columns=header_row)\n",
        "\n",
        "\n",
        "for yrs in range(2005,2006):\n",
        "    ratings_url = f\"https://www.sports-reference.com/cbb/seasons/men/{yrs}-ratings.html\" \n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    base_url = \"https://www.sports-reference.com/\"\n",
        "\n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    table = soup.find(\"table\", id=\"ratings\")\n",
        "    # rows = table.select(\"tbody tr\")\n",
        "    # rows = table.select(\"tbody tr[data-row]\")[:5]  # 첫 5개 행만 선택\n",
        "    rows = table.select(\"tbody tr\")[:2]  # 첫 5개 행만 선택\n",
        "\n",
        "    # 빈 데이터프레임 생성\n",
        "    combined_df = pd.DataFrame()\n",
        "   # 필요한 컬럼들을 리스트로 저장\n",
        "    columns_to_drop = [\"#\", \"Class\", \"Pos\", \"Height\", \"Weight\", \"Hometown\", \"High School\", \"RSCI Top 100\", \"Summary\"]\n",
        "\n",
        "    for row in rows:\n",
        "        link_element = row.select_one(\"td:nth-of-type(1) a\")\n",
        "        if link_element:\n",
        "            link = link_element[\"href\"]\n",
        "            full_url = base_url + link\n",
        "\n",
        "            response = requests.get(full_url)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            roster_table = soup.find(\"table\", id=\"roster\")\n",
        "            per_game_table = soup.find(\"table\", id=\"per_game\")\n",
        "            totals_table = soup.find(\"table\", id=\"totals\")\n",
        "            per_min_table = soup.find(\"table\", id=\"per_min\")\n",
        "            per_poss_table = soup.find(\"table\", id=\"per_poss\")\n",
        "            advanced_table = soup.find(\"table\", id=\"advanced\")\n",
        "\n",
        "            data_frames = list()\n",
        "\n",
        "            if roster_table is not None:\n",
        "                roster_data = get_table_data(roster_table)\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                roster_data[\"Season\"] = season\n",
        "                roster_data[\"School\"] = school\n",
        "                data_frames.append(roster_data)\n",
        "            else:\n",
        "                print(f\"{season} Roster table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_game_table is not None:\n",
        "                per_game_data = get_table_data(per_game_table)\n",
        "                per_game_data = per_game_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_game\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_game_data[\"Season\"] = season\n",
        "                per_game_data[\"School\"] = school\n",
        "                data_frames.append(per_game_data)\n",
        "            else:\n",
        "                print(f\"{season} Per Game table not found.\")\n",
        "                pass\n",
        "\n",
        "            if totals_table is not None:\n",
        "                totals_data = get_table_data(totals_table)\n",
        "                totals_data = totals_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_totals\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                totals_data[\"Season\"] = season\n",
        "                totals_data[\"School\"] = school\n",
        "                # print(\"Totals data frame:\")\n",
        "                # print(totals_data.head())\n",
        "                data_frames.append(totals_data)\n",
        "            else:\n",
        "                print(f\"{season} Totals table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_min_table is not None:\n",
        "                per_min_data = get_table_data(per_min_table)\n",
        "                per_min_data = per_min_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_min\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_min_data[\"Season\"] = season\n",
        "                per_min_data[\"School\"] = school\n",
        "                # print(\"Per 40 Min data frame:\")\n",
        "                # print(per_min_data.head())\n",
        "                data_frames.append(per_min_data)\n",
        "            else:\n",
        "                print(f\"{season} Per 40 Min table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_poss_table is not None:\n",
        "                per_poss_data = get_table_data(per_poss_table)\n",
        "                per_poss_data = per_poss_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_poss\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_poss_data[\"Season\"] = season\n",
        "                per_poss_data[\"School\"] = school\n",
        "                # print(\"Per 100 Poss data frame:\")\n",
        "                # print(per_poss_data.head())\n",
        "                data_frames.append(per_poss_data)\n",
        "            else:\n",
        "                print(f\"{season} Per 100 Poss table not found.\")\n",
        "                pass\n",
        "\n",
        "            if advanced_table is not None:\n",
        "                advanced_data = get_table_data(advanced_table)\n",
        "                advanced_data = advanced_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_advanced\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                advanced_data[\"Season\"] = season \n",
        "                advanced_data[\"School\"] = school\n",
        "                # print(\"Advanced data frame:\")\n",
        "                # print(advanced_data.head())\n",
        "                data_frames.append(advanced_data)\n",
        "            else:\n",
        "                print(f\"{season} Advanced table not found.\")\n",
        "                pass\n",
        "\n",
        "            combined_df = roster_data.copy()\n",
        "            for col in columns_to_drop:\n",
        "                try:\n",
        "                    combined_df = combined_df.drop(col, axis=1)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for df in data_frames:\n",
        "                if df is not None:\n",
        "                    df = df.loc[:, ~df.columns.duplicated()]\n",
        "                    combined_df = pd.merge(combined_df, df, on=[\"Player\", \"Season\", \"School\"], how=\"outer\")\n",
        "\n",
        "            combined_df = combined_df[[\"Season\", \"School\"] + [col for col in combined_df.columns if col not in [\"Season\", \"School\"]]]\n",
        "            # Season 값 형식 변경\n",
        "            # combined_df[\"Season\"] = combined_df[\"Season\"].apply(lambda x: f\"{x[:4]}-{x[-2:]}\")\n",
        "            combined_df[\"Season\"] = combined_df[\"Season\"].astype(str)\n",
        "\n",
        "            # 파일명 생성\n",
        "            filename = \"NCAA_\" + season + \".csv\"\n",
        "            #  # 빈 데이터프레임을 CSV 파일로 저장\n",
        "            # df.to_csv(filename, index=False)\n",
        "\n",
        "            # 파일이 이미 존재하는 경우\n",
        "            if filename in os.listdir():\n",
        "                # 기존 파일 읽어오기\n",
        "                existing_df = pd.read_csv(filename)\n",
        "                # 기존 데이터와 현재 데이터 병합\n",
        "                combined_df = pd.concat([existing_df, combined_df], ignore_index=False)\n",
        "            # else:\n",
        "                # combined_df = pd.concat([combined_df] + data_frames, ignore_index=False)\n",
        "\n",
        "            # CSV 파일로 저장\n",
        "            combined_df.to_csv(filename, index=False)\n",
        "            print(\"Combined DataFrame has been saved as\", filename)\n",
        "\n",
        "            # 일정 시간 대기\n",
        "            time.sleep(5)  # 5초 대기\n",
        "\n",
        "print(\"Crawling complete!\")"
      ],
      "metadata": {
        "id": "ZOt84h6W4uy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문2.\n",
        "1. 누구나 접속 가능한 웹페이지(ex. https://www.sports-reference.com/cbb/seasons/men/2022-ratings.html)\n",
        "\n",
        "2. 목적: 첫번째 페이지 내의 테이블(시즌별 학교순위)에서 school에 링크로 연결된 페이지를 방문하여 roster, total, per game 등의 테이블 데이터를 수집. 이 과정을 학교별 시즌별로 반복 수행. \n",
        "\n",
        "3. 문제상황: for yrs in range(season_from,season_to): 2019년처럼 최근의 시즌들에 대해서는 원하는 데이터를 가져오고 있으나 2000대 초반을 돌려보면 문제현상 발생.\n",
        "\n",
        "문제현상1. 컬럼명 Season의 표시형식이 연도가 아닌 월-일로 저장.\n",
        "\n",
        "문제현상2. 각각의 테이블을 데이터프레임으로 합치는 과정에서 combined_df = roster_data.copy()코드로 인해 roster_data가 중복으로 생성되는데 이에 대한 처리가 안됨: 컬럼명_x와 컬럼명_y가 생성되어 컬럼명_y들의 삭제처리와 컬럼명_x들의 \"_x\"삭제처리가 안됨."
      ],
      "metadata": {
        "id": "M5Lh5lj84vwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NCAA데이터 크롤링 완성\n",
        "#지정된 연도에 대한 모든 학교의 모든 선수의 모든 데이터 complete NCAA데이터 크롤링 완성\n",
        "\n",
        "#위의 코드 문제점\n",
        "#1. 2000년대 시즌 error: combined_df = combined_df.drop([\"#_y\", \"Class_y\", \"Pos_y\", \"Height_y\", \"Weight_y\", \"Hometown_y\", \"High School_y\", \"RSCI Top 100_y\", \"Summary_y\"], axis=1)\n",
        "#2. Season컬럼이 표시 형식이 2009-10시즌의 경우 Oct-09 식으로 표시됨.\n",
        "#3. 현재 1의 error는 drop할 컬럼명을 먼저 리스트로 만들어 놓고 컬럼명에 해당 컬럼이 있을 경우 삭제 처리하려고 시도 중이나 반영안되고 있음.\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import os\n",
        "\n",
        "def get_season_from_page(soup):\n",
        "    season_element = soup.select_one(\"#meta div h1 span\")\n",
        "    if season_element:\n",
        "        return season_element.text.strip().split()[0]\n",
        "    return \"\"\n",
        "\n",
        "def get_school_name(url):\n",
        "    url_parts = url.split(\"/\")\n",
        "    return url_parts[-3]\n",
        "\n",
        "\n",
        "def get_table_data(table):\n",
        "    header_row = [cell.text.strip() for cell in table.select(\"thead tr th\")]\n",
        "    data_rows = table.select(\"tbody tr\")\n",
        "    data = [[cell.text.strip() for cell in row.select(\"th, td\")] for row in data_rows]\n",
        "    return pd.DataFrame(data, columns=header_row)\n",
        "\n",
        "\n",
        "for yrs in range(2005,2007):\n",
        "    ratings_url = f\"https://www.sports-reference.com/cbb/seasons/men/{yrs}-ratings.html\" \n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    base_url = \"https://www.sports-reference.com/\"\n",
        "\n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    table = soup.find(\"table\", id=\"ratings\")\n",
        "    # rows = table.select(\"tbody tr\")\n",
        "    # rows = table.select(\"tbody tr[data-row]\")[:5]  # 첫 5개 행만 선택\n",
        "    rows = table.select(\"tbody tr\")[:2]  # 첫 5개 행만 선택\n",
        "\n",
        "    # 빈 데이터프레임 생성\n",
        "    combined_df = pd.DataFrame()\n",
        "   # 필요한 컬럼들을 리스트로 저장\n",
        "    columns_to_drop = [\"#_y\", \"Class_y\", \"Pos_y\", \"Height_y\", \"Weight_y\", \"Hometown_y\", \"High School_y\", \"RSCI Top 100_y\", \"Summary_y\"]\n",
        "\n",
        "    for row in rows:\n",
        "        link_element = row.select_one(\"td:nth-of-type(1) a\")\n",
        "        if link_element:\n",
        "            link = link_element[\"href\"]\n",
        "            full_url = base_url + link\n",
        "\n",
        "            response = requests.get(full_url)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            roster_table = soup.find(\"table\", id=\"roster\")\n",
        "            per_game_table = soup.find(\"table\", id=\"per_game\")\n",
        "            totals_table = soup.find(\"table\", id=\"totals\")\n",
        "            per_min_table = soup.find(\"table\", id=\"per_min\")\n",
        "            per_poss_table = soup.find(\"table\", id=\"per_poss\")\n",
        "            advanced_table = soup.find(\"table\", id=\"advanced\")\n",
        "\n",
        "            if roster_table is not None:\n",
        "                roster_data = get_table_data(roster_table)\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                roster_data[\"Season\"] = season\n",
        "                roster_data[\"School\"] = school\n",
        "            else:\n",
        "                print(f\"{season} Roster table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_game_table is not None:\n",
        "                per_game_data = get_table_data(per_game_table)\n",
        "                per_game_data = per_game_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_game\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_game_data[\"Season\"] = season\n",
        "                per_game_data[\"School\"] = school\n",
        "            else:\n",
        "                print(f\"{season} Per Game table not found.\")\n",
        "                pass\n",
        "\n",
        "            if totals_table is not None:\n",
        "                totals_data = get_table_data(totals_table)\n",
        "                totals_data = totals_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_totals\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                totals_data[\"Season\"] = season\n",
        "                totals_data[\"School\"] = school\n",
        "                # print(\"Totals data frame:\")\n",
        "                # print(totals_data.head())\n",
        "            else:\n",
        "                print(f\"{season} Totals table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_min_table is not None:\n",
        "                per_min_data = get_table_data(per_min_table)\n",
        "                per_min_data = per_min_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_min\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_min_data[\"Season\"] = season\n",
        "                per_min_data[\"School\"] = school\n",
        "                # print(\"Per 40 Min data frame:\")\n",
        "                # print(per_min_data.head())\n",
        "            else:\n",
        "                print(f\"{season} Per 40 Min table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_poss_table is not None:\n",
        "                per_poss_data = get_table_data(per_poss_table)\n",
        "                per_poss_data = per_poss_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_poss\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_poss_data[\"Season\"] = season\n",
        "                per_poss_data[\"School\"] = school\n",
        "                # print(\"Per 100 Poss data frame:\")\n",
        "                # print(per_poss_data.head())\n",
        "            else:\n",
        "                print(f\"{season} Per 100 Poss table not found.\")\n",
        "                pass\n",
        "\n",
        "            if advanced_table is not None:\n",
        "                advanced_data = get_table_data(advanced_table)\n",
        "                advanced_data = advanced_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_advanced\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                advanced_data[\"Season\"] = season \n",
        "                advanced_data[\"School\"] = school\n",
        "                # print(\"Advanced data frame:\")\n",
        "                # print(advanced_data.head())\n",
        "            else:\n",
        "                print(f\"{season} Advanced table not found.\")\n",
        "                pass\n",
        "\n",
        "            # data_frames = [roster_data, per_game_data]\n",
        "            # combined_df = pd.concat([combined_df] + data_frames, ignore_index=True, sort=False)\n",
        "\n",
        "\n",
        "            # Merge all data frames\n",
        "            data_frames = [roster_data, per_game_data, totals_data, per_min_data, per_poss_data, advanced_data]\n",
        "############################################################################################################################################################################### \n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################  \n",
        "#문제 부분 \n",
        "            combined_df = roster_data.copy()  \n",
        "\n",
        "\n",
        "            for df in data_frames:\n",
        "                if df is not None:\n",
        "                    df = df.loc[:, ~df.columns.duplicated()]\n",
        "                    combined_df = pd.merge(combined_df, df, on=[\"Player\", \"Season\", \"School\"], how=\"outer\")\n",
        "\n",
        "            # combined_df = combined_df.drop([\"#_y\", \"Class_y\", \"Pos_y\", \"Height_y\", \"Weight_y\", \"Hometown_y\", \"High School_y\", \"RSCI Top 100_y\", \"Summary_y\"], axis=1)\n",
        "            # combined_df.columns = combined_df.columns.str.replace(\"_x\", \"\")\n",
        "            # 컬럼들이 존재하는 경우에만 해당 컬럼 제거 및 이름 수정\n",
        "                    if all(col in combined_df.columns for col in columns_to_drop):\n",
        "                       combined_df = combined_df.drop(columns_to_drop, axis=1)\n",
        "                       combined_df.columns = combined_df.columns.str.replace(\"_x\", \"\")\n",
        "\n",
        "            combined_df = combined_df[[\"Season\", \"School\"] + [col for col in combined_df.columns if col not in [\"Season\", \"School\"]]]\n",
        "            # Season 값 형식 변경\n",
        "            # combined_df[\"Season\"] = combined_df[\"Season\"].apply(lambda x: f\"{x[:4]}-{x[-2:]}\")\n",
        "            combined_df[\"Season\"] = combined_df[\"Season\"].astype(str)\n",
        "############################################################################################################################################################################### \n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################  \n",
        "\n",
        "            # # 필요한 컬럼들을 리스트로 저장\n",
        "            # columns_to_drop = [\"#_y\", \"Class_y\", \"Pos_y\", \"Height_y\", \"Weight_y\", \"Hometown_y\", \"High School_y\", \"RSCI Top 100_y\", \"Summary_y\"]\n",
        "\n",
        "\n",
        "            # 파일명 생성\n",
        "            filename = \"NCAA_\" + season + \".csv\"\n",
        "            #  # 빈 데이터프레임을 CSV 파일로 저장\n",
        "            # df.to_csv(filename, index=False)\n",
        "\n",
        "            # 파일이 이미 존재하는 경우\n",
        "            if filename in os.listdir():\n",
        "                # 기존 파일 읽어오기\n",
        "                existing_df = pd.read_csv(filename)\n",
        "                # 기존 데이터와 현재 데이터 병합\n",
        "                combined_df = pd.concat([existing_df, combined_df], ignore_index=False)\n",
        "            # else:\n",
        "                # combined_df = pd.concat([combined_df] + data_frames, ignore_index=False)\n",
        "\n",
        "            # CSV 파일로 저장\n",
        "            combined_df.to_csv(filename, index=False)\n",
        "            print(\"Combined DataFrame has been saved as\", filename)\n",
        "\n",
        "            # 일정 시간 대기\n",
        "            time.sleep(5)  # 5초 대기\n",
        "\n",
        "print(\"Crawling complete!\")"
      ],
      "metadata": {
        "id": "yJ-4lYwa4v-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}