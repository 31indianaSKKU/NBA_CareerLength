{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKm1m9DSa4Z1sb0jRv8XaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/31indianaSKKU/NBA_CareerLength/blob/main/%E2%98%85CareerLength_Stathead_com_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 문제상황: 링크로 연결된 url로 가서 데이터를 가져 오고 있지만 다시 처음의 페이지로 돌아와 다음 행의 선수 링크 url로 이동하지 못하고 있음. (에러 첨부)\n",
        "-> 여러가지 문제가 복합적으로 있어서 좀 많이 고쳤습니다.\n",
        "-> 일단 매번 뒤로가기 하는건 속도측면에서 비효율적이라. url먼저 수집해두고 도는 방식으로 수정 했습니다.\n",
        "-> 선수 페이지로 넘어가면 도메인이 변경되면서 로그인이 풀리는 이슈가 있어 로그인 로직을 추가 했습니다.\n",
        "-> 페이지 넘기는 기능은 offset값만 바꾸는 방식으로 변경 했습니다.\n",
        "-> 안타깝게도... colab에서 undetected_chromedriver를 정상동작 시키는 방법을 찾지 못해서 주피터에서만 동작하는 코드를 공유 드립니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "oQUEpYcZcEbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install undetected_chromedriver\n",
        "!pip install webdriver_manager\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "oXs5Mk1-cFCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#수정 후 코드\n",
        "#쥬피터 OK\n",
        "#draft list의 선수별 데이터 가져오기(career length)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import os\n",
        "\n",
        "def get_season_from_page(soup):\n",
        "    season_element = soup.select_one(\"#meta div h1 span\")\n",
        "    if season_element:\n",
        "        return season_element.text.strip().split()[0]\n",
        "    return \"\"\n",
        "\n",
        "def get_school_name(url):\n",
        "    url_parts = url.split(\"/\")\n",
        "    return url_parts[-3]\n",
        "\n",
        "\n",
        "def get_table_data(table):\n",
        "    header_row = [cell.text.strip() for cell in table.select(\"thead tr th\")]\n",
        "    data_rows = table.select(\"tbody tr\")\n",
        "    data = [[cell.text.strip() for cell in row.select(\"th, td\")] for row in data_rows]\n",
        "    return pd.DataFrame(data, columns=header_row)\n",
        "\n",
        "\n",
        "for yrs in range(2000,2001):\n",
        "    ratings_url = f\"https://www.sports-reference.com/cbb/seasons/men/{yrs}-ratings.html\" \n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    base_url = \"https://www.sports-reference.com/\"\n",
        "\n",
        "    response = requests.get(ratings_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    table = soup.find(\"table\", id=\"ratings\")\n",
        "#     rows = table.select(\"tbody tr\")\n",
        "    # rows = table.select(\"tbody tr[data-row]\")[:5]  # 첫 5개 행만 선택\n",
        "    rows = table.select(\"tbody tr\")[:2]  # 첫 5개 행만 선택\n",
        "\n",
        "    # 빈 데이터프레임 생성\n",
        "    combined_df = pd.DataFrame()\n",
        "   # 필요한 컬럼들을 리스트로 저장\n",
        "    columns_to_drop = [\"#\", \"Class\", \"Pos\", \"Height\", \"Weight\", \"Hometown\", \"High School\", \"RSCI Top 100\", \"Summary\"]\n",
        "\n",
        "    for row in rows:\n",
        "        link_element = row.select_one(\"td:nth-of-type(1) a\")\n",
        "        if link_element:\n",
        "            link = link_element[\"href\"]\n",
        "            full_url = base_url + link\n",
        "\n",
        "            response = requests.get(full_url)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            roster_table = soup.find(\"table\", id=\"roster\")\n",
        "            per_game_table = soup.find(\"table\", id=\"per_game\")\n",
        "            totals_table = soup.find(\"table\", id=\"totals\")\n",
        "            per_min_table = soup.find(\"table\", id=\"per_min\")\n",
        "            per_poss_table = soup.find(\"table\", id=\"per_poss\")\n",
        "            advanced_table = soup.find(\"table\", id=\"advanced\")\n",
        "\n",
        "            data_frames = list()\n",
        "\n",
        "            if roster_table is not None:\n",
        "                roster_data = get_table_data(roster_table)\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                roster_data[\"Season\"] = season\n",
        "                roster_data[\"School\"] = school\n",
        "                data_frames.append(roster_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Roster table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_game_table is not None:\n",
        "                per_game_data = get_table_data(per_game_table)\n",
        "                per_game_data = per_game_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_game\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_game_data[\"Season\"] = season\n",
        "                per_game_data[\"School\"] = school\n",
        "                data_frames.append(per_game_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} PerGame table not found.\")\n",
        "                pass\n",
        "\n",
        "            if totals_table is not None:\n",
        "                totals_data = get_table_data(totals_table)\n",
        "                totals_data = totals_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_totals\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                totals_data[\"Season\"] = season\n",
        "                totals_data[\"School\"] = school\n",
        "                # print(\"Totals data frame:\")\n",
        "                # print(totals_data.head())\n",
        "                data_frames.append(totals_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Totals table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_min_table is not None:\n",
        "                per_min_data = get_table_data(per_min_table)\n",
        "                per_min_data = per_min_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_min\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_min_data[\"Season\"] = season\n",
        "                per_min_data[\"School\"] = school\n",
        "                # print(\"Per 40 Min data frame:\")\n",
        "                # print(per_min_data.head())\n",
        "                data_frames.append(per_min_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Per40Min table not found.\")\n",
        "                pass\n",
        "\n",
        "            if per_poss_table is not None:\n",
        "                per_poss_data = get_table_data(per_poss_table)\n",
        "                per_poss_data = per_poss_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_per_poss\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                per_poss_data[\"Season\"] = season\n",
        "                per_poss_data[\"School\"] = school\n",
        "                # print(\"Per 100 Poss data frame:\")\n",
        "                # print(per_poss_data.head())\n",
        "                data_frames.append(per_poss_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Per100Poss table not found.\")\n",
        "                pass\n",
        "\n",
        "            if advanced_table is not None:\n",
        "                advanced_data = get_table_data(advanced_table)\n",
        "                advanced_data = advanced_data.rename(columns=lambda x: x if x in [\"Player\", \"Season\", \"School\"] else x + \"_advanced\")\n",
        "                season = get_season_from_page(soup)\n",
        "                school = get_school_name(full_url)\n",
        "                advanced_data[\"Season\"] = season \n",
        "                advanced_data[\"School\"] = school\n",
        "                # print(\"Advanced data frame:\")\n",
        "                # print(advanced_data.head())\n",
        "                data_frames.append(advanced_data)\n",
        "            else:\n",
        "                print(f\"{season} {school} Advanced table not found.\")\n",
        "                pass\n",
        "\n",
        "            combined_df = roster_data.copy()\n",
        "            for col in columns_to_drop:\n",
        "                try:\n",
        "                    combined_df = combined_df.drop(col, axis=1)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for df in data_frames:\n",
        "                if df is not None:\n",
        "                    df = df.loc[:, ~df.columns.duplicated()]\n",
        "                    combined_df = pd.merge(combined_df, df, on=[\"Player\", \"Season\", \"School\"], how=\"outer\")\n",
        "\n",
        "            combined_df = combined_df[[\"Season\", \"School\"] + [col for col in combined_df.columns if col not in [\"Season\", \"School\"]]]\n",
        "            # Season 값 형식 변경\n",
        "            # combined_df[\"Season\"] = combined_df[\"Season\"].apply(lambda x: f\"{x[:4]}-{x[-2:]}\")\n",
        "            combined_df[\"Season\"] = combined_df[\"Season\"].astype(str)\n",
        "\n",
        "            # 파일명 생성\n",
        "            filename = \"NCAA_\" + season + \".csv\"\n",
        "            #  # 빈 데이터프레임을 CSV 파일로 저장\n",
        "            # df.to_csv(filename, index=False)\n",
        "\n",
        "            # 파일이 이미 존재하는 경우\n",
        "            if filename in os.listdir():\n",
        "                # 기존 파일 읽어오기\n",
        "                existing_df = pd.read_csv(filename)\n",
        "                # 기존 데이터와 현재 데이터 병합\n",
        "                combined_df = pd.concat([existing_df, combined_df], ignore_index=False)\n",
        "            # else:\n",
        "                # combined_df = pd.concat([combined_df] + data_frames, ignore_index=False)\n",
        "\n",
        "            # CSV 파일로 저장\n",
        "            combined_df.to_csv(filename, index=False)\n",
        "            print(\"Combined DataFrame has been saved as\", filename)\n",
        "\n",
        "            # 일정 시간 대기\n",
        "            time.sleep(5)  # 5초 대기\n",
        "\n",
        "print(\"Crawling complete!\")\n",
        "    "
      ],
      "metadata": {
        "id": "sJjbFZk7cJ7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문3.\n",
        "\n",
        "1. 해당 웹페이지는 구글계정 로그인이 필요한 페이지입니다.(유료사이트)\n",
        "\n",
        "2. 목적: 첫번째 접속 페이지 내의 테이블안에 있는 player에 링크로 연결된 url로 이동 후 데이터 수집\n",
        "\n",
        "3. 문제상황: 링크로 연결된 url로 가서 데이터를 가져 오고 있지만 다시 처음의 페이지로 돌아와 다음 행의 선수 링크 url로 이동하지 못하고 있음. (에러 첨부)\n"
      ],
      "metadata": {
        "id": "9PBa1gpscFIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#수정 전 코드\n",
        "#draft list의 선수별 데이터 가져오기(career length)\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import pandas as pd\n",
        "\n",
        "# # sheet 생성\n",
        "# wb = openpyxl.Workbook()\n",
        "# sheet = wb.active\n",
        "# sheet.append([\"no\", \"displayName\", \"team\", \"officialPosition\", \"averageSalary\"])\n",
        "\n",
        "\n",
        "def init_driver():\n",
        "    browser = uc.Chrome()\n",
        "    browser.get('https://velog.io')\n",
        "    return browser\n",
        "\n",
        "# 로그인 버튼을 눌러주고 로그인이 되서 다시 velog로 돌아올때까지 60초를 기다린다.\n",
        "def do_login(browser):\n",
        "    browser.find_element(By.XPATH,'//button[text()=\"로그인\"]').click()\n",
        "  \n",
        "    google_login = browser.find_element(By.XPATH, '//*[@id=\"root\"]/div[4]/div/div[2]/div[2]/div/div[1]/section[2]/div/a[2]').get_attribute('href')         \n",
        "    browser.get(google_login)\n",
        "    email_input = browser.find_element(By.XPATH, '//*[@id=\"identifierId\"]')\n",
        "    email_input.send_keys('구글계정')\n",
        "    browser.find_element(By.XPATH, '//*[@id=\"identifierNext\"]/div/button').click()\n",
        "    time.sleep(7)\n",
        "    password_input = browser.find_element(By.XPATH, '//*[@id=\"password\"]/div[1]/div/div[1]/input')\n",
        "    password_input.send_keys('구글계정비번')     \n",
        "    browser.find_element(By.XPATH, '//*[@id=\"passwordNext\"]/div/button').click()\n",
        "    time.sleep(15)\n",
        "    \n",
        "if  __name__  ==  \"__main__\" :\n",
        "    browser = init_driver()\n",
        "    do_login(browser)\n",
        "    url = 'https://stathead.com/basketball/draft_finder.cgi?request=1&order_by_asc=1&is_active=N&order_by=year_id&year_max=2020&year_min=2000&offset=0'\n",
        "\n",
        "    while True:\n",
        "        # Load the page\n",
        "        browser.get(url)\n",
        "        target_element_login = browser.find_element(By.XPATH, '//*[@id=\"subnav\"]/li[12]/a')\n",
        "        target_element_login.click()\n",
        "        target_element_login2 = browser.find_element(By.XPATH, '//*[@id=\"sh-login-google\"]')\n",
        "        target_element_login2.click()\n",
        "        browser.get(url)\n",
        "        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'stats')))  # Wait for the table to load\n",
        "\n",
        "        # Extract the data from the table\n",
        "        table = browser.find_element(By.XPATH, '//*[@id=\"stats\"]')\n",
        "        trs = table.find_elements(By.XPATH, './/tr')\n",
        "        \n",
        "#         rows = table.find_elements(By.XPATH, './/tr')[0:]  # Exclude header row   \n",
        "        \n",
        "        data_list = []\n",
        "\n",
        "\n",
        "        for tr in trs:        \n",
        "            try:\n",
        "                url_cell = tr.find_element(By.XPATH, '//*[@id=\"stats\"]/tbody/tr[1]/td[5]')\n",
        "                player_url = url_cell.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
        "#             except StaleElementReferenceException:\n",
        "                \n",
        "                browser.get(player_url)\n",
        "                try:\n",
        "                    more_bio_button = browser.find_element(By.XPATH, '//*[@id=\"meta_more_button\"]')\n",
        "                    more_bio_button.click()\n",
        "                except NoSuchElementException:\n",
        "                    pass  # Continue without clicking if \"More Bio\" button is not found             \n",
        "             # Extract the data from the player page\n",
        "                data_element = browser.find_element(By.CSS_SELECTOR, '#meta div:nth-of-type(2)')\n",
        "                data = data_element.text.strip()\n",
        "                data_list.append(data)\n",
        "                print(data_list)\n",
        "                time.sleep(5)\n",
        "############################################################################################################################################################################### \n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################  \n",
        "#문제 부분                 \n",
        "                # Go back to the previous page\n",
        "                browser.back()   #//*[@id=\"stats\"]/tbody/tr[2]/td[5]\n",
        "                time.sleep(5)\n",
        "                continue               \n",
        "                \n",
        "            except NoSuchElementException:\n",
        "                continue\n",
        "        \n",
        "        \n",
        "        df = pd.DataFrame(data_list)\n",
        "        \n",
        "        # Print the DataFrame\n",
        "        print(df)\n",
        "\n",
        "        # Check if there is a next page\n",
        "        next_button = browser.find_element(By.CSS_SELECTOR, '#stathead_results div:last-of-type a')\n",
        "        if not next_button:\n",
        "            break\n",
        "\n",
        "        # Get the URL of the next page\n",
        "        next_url = next_button.get_attribute('href')\n",
        "\n",
        "        # Build the full URL for the next page\n",
        "        url = f'https://stathead.com{next_url}'\n",
        "\n",
        "    # Step 4: Process is complete\n",
        "    print(\"Crawling complete!\")\n",
        "\n",
        "    # Quit the WebDriver\n",
        "    driver.quit()\n",
        "    "
      ],
      "metadata": {
        "id": "r-oNBDdHcFOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}