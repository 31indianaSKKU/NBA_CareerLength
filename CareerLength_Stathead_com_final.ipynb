{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEsklnDEkLkvC2Xly2cjIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/31indianaSKKU/NBA_CareerLength/blob/main/CareerLength_Stathead_com_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 문제상황: 링크로 연결된 url로 가서 데이터를 가져 오고 있지만 다시 처음의 페이지로 돌아와 다음 행의 선수 링크 url로 이동하지 못하고 있음. (에러 첨부)\n",
        "-> 여러가지 문제가 복합적으로 있어서 좀 많이 고쳤습니다.\n",
        "-> 일단 매번 뒤로가기 하는건 속도측면에서 비효율적이라. url먼저 수집해두고 도는 방식으로 수정 했습니다.\n",
        "-> 선수 페이지로 넘어가면 도메인이 변경되면서 로그인이 풀리는 이슈가 있어 로그인 로직을 추가 했습니다.\n",
        "-> 페이지 넘기는 기능은 offset값만 바꾸는 방식으로 변경 했습니다.\n",
        "-> 안타깝게도... colab에서 undetected_chromedriver를 정상동작 시키는 방법을 찾지 못해서 주피터에서만 동작하는 코드를 공유 드립니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "oQUEpYcZcEbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install undetected_chromedriver\n",
        "!pip install webdriver_manager\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "oXs5Mk1-cFCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#수정 후 코드\n",
        "#쥬피터 OK\n",
        "#draft list의 선수별 데이터 가져오기(career length)\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import pandas as pd\n",
        "\n",
        "# # sheet 생성\n",
        "# wb = openpyxl.Workbook()\n",
        "# sheet = wb.active\n",
        "# sheet.append([\"no\", \"displayName\", \"team\", \"officialPosition\", \"averageSalary\"])\n",
        "\n",
        "\n",
        "def init_driver():\n",
        "    browser = uc.Chrome()\n",
        "    browser.get('https://velog.io')\n",
        "    return browser\n",
        "\n",
        "# 로그인 버튼을 눌러주고 로그인이 되서 다시 velog로 돌아올때까지 60초를 기다린다.\n",
        "def do_login(browser):\n",
        "    browser.find_element(By.XPATH,'//button[text()=\"로그인\"]').click()\n",
        "  \n",
        "    google_login = browser.find_element(By.XPATH, '//*[@id=\"root\"]/div[4]/div/div[2]/div[2]/div/div[1]/section[2]/div/a[2]').get_attribute('href')         \n",
        "    browser.get(google_login)\n",
        "    email_input = browser.find_element(By.XPATH, '//*[@id=\"identifierId\"]')\n",
        "    email_input.send_keys('jungmu.test')\n",
        "    browser.find_element(By.XPATH, '//*[@id=\"identifierNext\"]/div/button').click()\n",
        "    time.sleep(5)\n",
        "    password_input = browser.find_element(By.XPATH, '//*[@id=\"password\"]/div[1]/div/div[1]/input')\n",
        "    password_input.send_keys('quf25468!')     \n",
        "    browser.find_element(By.XPATH, '//*[@id=\"passwordNext\"]/div/button').click()\n",
        "    time.sleep(5)\n",
        "    \n",
        "if  __name__  ==  \"__main__\" :\n",
        "    browser = init_driver()\n",
        "    do_login(browser)\n",
        "    url = 'https://stathead.com/basketball/draft_finder.cgi?request=1&order_by_asc=1&is_active=N&order_by=year_id&year_max=2020&year_min=2000&offset='\n",
        "\n",
        "    loop_count = 0\n",
        "    \n",
        "    # Load the page\n",
        "    browser.get(f'{url}{loop_count * 100}')\n",
        "    target_element_login = browser.find_element(By.XPATH, '//*[@id=\"subnav\"]/li[12]/a')\n",
        "    target_element_login.click()\n",
        "    target_element_login2 = browser.find_element(By.XPATH, '//*[@id=\"sh-login-google\"]')\n",
        "    target_element_login2.click()\n",
        "    browser.get(f'{url}{loop_count * 100}')\n",
        "    \n",
        "    is_complete_login = False\n",
        "    \n",
        "    while True:\n",
        "        print(\"loop\")\n",
        "        loop_count += 1\n",
        "        print(loop_count)\n",
        "        \n",
        "        WebDriverWait(browser, 5).until(EC.presence_of_element_located((By.ID, 'stats')))  # Wait for the table to load\n",
        "\n",
        "        # Extract the data from the table\n",
        "        table = browser.find_element(By.XPATH, '//*[@id=\"stats\"]')\n",
        "        trs = table.find_elements(By.XPATH, './/tr')\n",
        "        \n",
        "#         rows = table.find_elements(By.XPATH, './/tr')[0:]  # Exclude header row   \n",
        "        \n",
        "        data_list = []\n",
        "\n",
        "        link_list = []\n",
        "        \n",
        "        for tr in trs:\n",
        "            try:                  \n",
        "                url_cell = tr.find_elements(By.XPATH, './/td')[4]\n",
        "                print(url_cell.text)\n",
        "                player_url = url_cell.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
        "                link_list.append(player_url.strip())\n",
        "                \n",
        "            except Exception as e:\n",
        "                continue\n",
        "                \n",
        "        if(is_complete_login == False):\n",
        "            new_login_url = 'https://www.basketball-reference.com/'\n",
        "            browser.get(link)\n",
        "            time.sleep(3)\n",
        "            target_element_login = browser.find_element(By.XPATH, '//*[@id=\"subnav\"]/li[12]/a')\n",
        "            target_element_login.click()\n",
        "            time.sleep(3)\n",
        "            is_complete_login = True\n",
        "                \n",
        "        for link in link_list:\n",
        "            print(link)\n",
        "            browser.get(link)\n",
        "            time.sleep(5)\n",
        "            try:\n",
        "                more_bio_button = browser.find_element(By.XPATH, '//*[@id=\"meta_more_button\"]')\n",
        "                more_bio_button.click()\n",
        "            except Exception as e:\n",
        "                pass  # Continue without clicking if \"More Bio\" button is not found             \n",
        "            # Extract the data from the player page\n",
        "            data_element = browser.find_element(By.CLASS_NAME, 'stats_pullout')\n",
        "            data = data_element.text.strip()\n",
        "            data_list.append(data)\n",
        "            print(data)\n",
        "\n",
        "        browser.get(f'{url}{loop_count * 100}')\n",
        "        time.sleep(5)\n",
        "        \n",
        "        df = pd.DataFrame(data_list)\n",
        "        \n",
        "        # Print the DataFrame\n",
        "        print(df)\n",
        "        \n",
        "        try:\n",
        "            next_button = browser.find_element(By.CLASS_NAME, 'next')\n",
        "        except:\n",
        "            break\n",
        "\n",
        "\n",
        "    # Step 4: Process is complete\n",
        "    print(\"Crawling complete!\")\n",
        "\n",
        "    # Quit the WebDriver\n",
        "    browser.quit()\n",
        "    "
      ],
      "metadata": {
        "id": "sJjbFZk7cJ7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문3.\n",
        "\n",
        "1. 해당 웹페이지는 구글계정 로그인이 필요한 페이지입니다.(유료사이트)\n",
        "\n",
        "2. 목적: 첫번째 접속 페이지 내의 테이블안에 있는 player에 링크로 연결된 url로 이동 후 데이터 수집\n",
        "\n",
        "3. 문제상황: 링크로 연결된 url로 가서 데이터를 가져 오고 있지만 다시 처음의 페이지로 돌아와 다음 행의 선수 링크 url로 이동하지 못하고 있음. (에러 첨부)\n"
      ],
      "metadata": {
        "id": "9PBa1gpscFIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#수정 전 코드\n",
        "#draft list의 선수별 데이터 가져오기(career length)\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import pandas as pd\n",
        "\n",
        "# # sheet 생성\n",
        "# wb = openpyxl.Workbook()\n",
        "# sheet = wb.active\n",
        "# sheet.append([\"no\", \"displayName\", \"team\", \"officialPosition\", \"averageSalary\"])\n",
        "\n",
        "\n",
        "def init_driver():\n",
        "    browser = uc.Chrome()\n",
        "    browser.get('https://velog.io')\n",
        "    return browser\n",
        "\n",
        "# 로그인 버튼을 눌러주고 로그인이 되서 다시 velog로 돌아올때까지 60초를 기다린다.\n",
        "def do_login(browser):\n",
        "    browser.find_element(By.XPATH,'//button[text()=\"로그인\"]').click()\n",
        "  \n",
        "    google_login = browser.find_element(By.XPATH, '//*[@id=\"root\"]/div[4]/div/div[2]/div[2]/div/div[1]/section[2]/div/a[2]').get_attribute('href')         \n",
        "    browser.get(google_login)\n",
        "    email_input = browser.find_element(By.XPATH, '//*[@id=\"identifierId\"]')\n",
        "    email_input.send_keys('구글계정')\n",
        "    browser.find_element(By.XPATH, '//*[@id=\"identifierNext\"]/div/button').click()\n",
        "    time.sleep(7)\n",
        "    password_input = browser.find_element(By.XPATH, '//*[@id=\"password\"]/div[1]/div/div[1]/input')\n",
        "    password_input.send_keys('구글계정비번')     \n",
        "    browser.find_element(By.XPATH, '//*[@id=\"passwordNext\"]/div/button').click()\n",
        "    time.sleep(15)\n",
        "    \n",
        "if  __name__  ==  \"__main__\" :\n",
        "    browser = init_driver()\n",
        "    do_login(browser)\n",
        "    url = 'https://stathead.com/basketball/draft_finder.cgi?request=1&order_by_asc=1&is_active=N&order_by=year_id&year_max=2020&year_min=2000&offset=0'\n",
        "\n",
        "    while True:\n",
        "        # Load the page\n",
        "        browser.get(url)\n",
        "        target_element_login = browser.find_element(By.XPATH, '//*[@id=\"subnav\"]/li[12]/a')\n",
        "        target_element_login.click()\n",
        "        target_element_login2 = browser.find_element(By.XPATH, '//*[@id=\"sh-login-google\"]')\n",
        "        target_element_login2.click()\n",
        "        browser.get(url)\n",
        "        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'stats')))  # Wait for the table to load\n",
        "\n",
        "        # Extract the data from the table\n",
        "        table = browser.find_element(By.XPATH, '//*[@id=\"stats\"]')\n",
        "        trs = table.find_elements(By.XPATH, './/tr')\n",
        "        \n",
        "#         rows = table.find_elements(By.XPATH, './/tr')[0:]  # Exclude header row   \n",
        "        \n",
        "        data_list = []\n",
        "\n",
        "\n",
        "        for tr in trs:        \n",
        "            try:\n",
        "                url_cell = tr.find_element(By.XPATH, '//*[@id=\"stats\"]/tbody/tr[1]/td[5]')\n",
        "                player_url = url_cell.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
        "#             except StaleElementReferenceException:\n",
        "                \n",
        "                browser.get(player_url)\n",
        "                try:\n",
        "                    more_bio_button = browser.find_element(By.XPATH, '//*[@id=\"meta_more_button\"]')\n",
        "                    more_bio_button.click()\n",
        "                except NoSuchElementException:\n",
        "                    pass  # Continue without clicking if \"More Bio\" button is not found             \n",
        "             # Extract the data from the player page\n",
        "                data_element = browser.find_element(By.CSS_SELECTOR, '#meta div:nth-of-type(2)')\n",
        "                data = data_element.text.strip()\n",
        "                data_list.append(data)\n",
        "                print(data_list)\n",
        "                time.sleep(5)\n",
        "############################################################################################################################################################################### \n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################\n",
        "###############################################################################################################################################################################  \n",
        "#문제 부분                 \n",
        "                # Go back to the previous page\n",
        "                browser.back()   #//*[@id=\"stats\"]/tbody/tr[2]/td[5]\n",
        "                time.sleep(5)\n",
        "                continue               \n",
        "                \n",
        "            except NoSuchElementException:\n",
        "                continue\n",
        "        \n",
        "        \n",
        "        df = pd.DataFrame(data_list)\n",
        "        \n",
        "        # Print the DataFrame\n",
        "        print(df)\n",
        "\n",
        "        # Check if there is a next page\n",
        "        next_button = browser.find_element(By.CSS_SELECTOR, '#stathead_results div:last-of-type a')\n",
        "        if not next_button:\n",
        "            break\n",
        "\n",
        "        # Get the URL of the next page\n",
        "        next_url = next_button.get_attribute('href')\n",
        "\n",
        "        # Build the full URL for the next page\n",
        "        url = f'https://stathead.com{next_url}'\n",
        "\n",
        "    # Step 4: Process is complete\n",
        "    print(\"Crawling complete!\")\n",
        "\n",
        "    # Quit the WebDriver\n",
        "    driver.quit()\n",
        "    "
      ],
      "metadata": {
        "id": "r-oNBDdHcFOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}